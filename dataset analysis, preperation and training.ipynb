{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec68397a",
   "metadata": {},
   "source": [
    "Opis:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c4b68",
   "metadata": {},
   "source": [
    "Generisanje datafrejma - funkcije za pripremu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41067b23-f757-482c-a75f-fc3108c9b4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd4c4af-8a78-42a3-a23d-b8e9f6e22c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(annotations_folder_path):\n",
    "    annotations = []\n",
    "    for annotation_name in tqdm(os.listdir(annotations_folder_path)):\n",
    "        annotation_path = os.path.join(annotations_folder_path, annotation_name)\n",
    "        annotation = json.load(open(annotation_path))\n",
    "        annotation[\"filename\"] = annotation_name.replace(\".json\", \"\")\n",
    "        annotations.append(annotation)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5766df5-b573-48b4-8592-c6b14629d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_dict(input_dict, base_key=\"\", output_dict={}, index_value=None):\n",
    "    \n",
    "    for key in input_dict.keys():\n",
    "        \n",
    "        if base_key == \"\":\n",
    "            full_key = key     \n",
    "        else:\n",
    "            if index_value is None:\n",
    "                full_key = base_key + \"/\" + key\n",
    "            else:\n",
    "                full_key = base_key + \"/\" + key + \"/\" + index_value\n",
    "\n",
    "        if isinstance(input_dict[key], dict):\n",
    "            get_flattened_dict(input_dict[key], full_key, output_dict)\n",
    "            \n",
    "        elif isinstance(input_dict[key], list):\n",
    "            for index, item in enumerate(input_dict[key]):\n",
    "                full_list_key = full_key\n",
    "                get_flattened_dict(item, full_list_key, output_dict, str(index))\n",
    "                    \n",
    "        else:\n",
    "            output_dict[full_key] = input_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5005aea6-8e89-4ef1-ad86-abaaa439fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_ORDER_DICT = {\n",
    "    \"description\": \"00-00000-00\",\n",
    "    \"tags\":[],\n",
    "    \"size\": {\n",
    "        \"width\": \"02-0000-00\",\n",
    "        \"height\": \"03-0000-00\"\n",
    "    },\n",
    "    \"objects\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46cd044-ab9c-4dd4-8e2a-18aeb7edc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_index(split_key, key_order_dict, level=0, id_value=None):\n",
    "    key = split_key[level]\n",
    "   \n",
    "        \n",
    "    if isinstance(key_order_dict[key], dict):\n",
    "        return get_column_index(split_key, key_order_dict[key], level+1, id_value)\n",
    "    \n",
    "    else:\n",
    "        column_index = key_order_dict[key]\n",
    "        if id_value is not None:\n",
    "            split_column_index = column_index.split(\"-\")\n",
    "            split_column_index[1] = split_column_index[1].replace(\"id\", id_value)[-5:]\n",
    "            column_index = \"-\".join(split_column_index)\n",
    "        return column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc4bc4e-4b34-42ff-9a44-872a98ed0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sorted_columns(annotation_df_columns):\n",
    "    sorted_annotation_df_cols = []\n",
    "    for annotation_df_col in annotation_df_columns:\n",
    "        split_col = annotation_df_col.split(\"/\")\n",
    "        col_index = get_column_index(split_col, KEY_ORDER_DICT, 0)\n",
    "        sorted_annotation_df_cols.append(str(col_index) + \"#\" + annotation_df_col)\n",
    "    sorted_annotation_df_cols.sort()\n",
    "    sorted_annotation_df_cols = [sorted_annotation_df_col.split(\"#\")[1] for sorted_annotation_df_col in sorted_annotation_df_cols]\n",
    "    return sorted_annotation_df_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ad05be-5afa-410b-806b-4488c944975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_df(annotations):\n",
    "    annotation_dicts = []\n",
    "    for annotation in annotations:\n",
    "        annotation_dict = {}\n",
    "        get_flattened_dict(annotation, \"\", annotation_dict)\n",
    "        annotation_dicts.append(annotation_dict)\n",
    "    annotation_dicts_keys = set(list(itertools.chain.from_iterable([list(annotation_dict.keys()) for annotation_dict in annotation_dicts])))\n",
    "    annotation_dicts_keys = get_sorted_columns(list(annotation_dicts_keys))\n",
    "    annotation_df_dict = {key: [] for key in annotation_dicts_keys}\n",
    "    for annotation_dict in annotation_dicts:\n",
    "        for key in annotation_df_dict:\n",
    "            value = annotation_dict[key] if key in annotation_dict else np.nan\n",
    "            annotation_df_dict[key].append(value)\n",
    "    json_df = pd.DataFrame(annotation_df_dict)\n",
    "    return json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3571a",
   "metadata": {},
   "source": [
    "Generisanje datafrejma - sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f2c68f-232c-4aa0-b129-19319ed3e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 432.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>size</th>\n",
       "      <th>objects</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10789675, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 88945149, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>b1f6c103-8b75ea3e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10790092, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 88948965, 'classId': 6508804, 'descrip...</td>\n",
       "      <td>b25fb716-78d8d49b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10790221, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 88950127, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>b26f9b1e-3e172ced.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10790369, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 88952159, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>b289f2e8-cec8700d.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10791039, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 88959006, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>b313a4dc-217f1233.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10818874, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 89256297, 'classId': 6508804, 'descrip...</td>\n",
       "      <td>c9b9fef4-a58bf3d5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10818931, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 89256888, 'classId': 6508800, 'descrip...</td>\n",
       "      <td>c9c31e57-45b42452.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10819441, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 89261734, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>ca28f275-03f8cef5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10819429, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 89261594, 'classId': 6508807, 'descrip...</td>\n",
       "      <td>ca28f275-cac4f254.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td></td>\n",
       "      <td>[{'id': 10819405, 'tagId': 26739, 'name': 'wea...</td>\n",
       "      <td>{'height': 720, 'width': 1280}</td>\n",
       "      <td>[{'id': 89261361, 'classId': 6508800, 'descrip...</td>\n",
       "      <td>ca2ba838-fdddb1fb.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    description                                               tags  \\\n",
       "0                [{'id': 10789675, 'tagId': 26739, 'name': 'wea...   \n",
       "1                [{'id': 10790092, 'tagId': 26739, 'name': 'wea...   \n",
       "2                [{'id': 10790221, 'tagId': 26739, 'name': 'wea...   \n",
       "3                [{'id': 10790369, 'tagId': 26739, 'name': 'wea...   \n",
       "4                [{'id': 10791039, 'tagId': 26739, 'name': 'wea...   \n",
       "..          ...                                                ...   \n",
       "104              [{'id': 10818874, 'tagId': 26739, 'name': 'wea...   \n",
       "105              [{'id': 10818931, 'tagId': 26739, 'name': 'wea...   \n",
       "106              [{'id': 10819441, 'tagId': 26739, 'name': 'wea...   \n",
       "107              [{'id': 10819429, 'tagId': 26739, 'name': 'wea...   \n",
       "108              [{'id': 10819405, 'tagId': 26739, 'name': 'wea...   \n",
       "\n",
       "                               size  \\\n",
       "0    {'height': 720, 'width': 1280}   \n",
       "1    {'height': 720, 'width': 1280}   \n",
       "2    {'height': 720, 'width': 1280}   \n",
       "3    {'height': 720, 'width': 1280}   \n",
       "4    {'height': 720, 'width': 1280}   \n",
       "..                              ...   \n",
       "104  {'height': 720, 'width': 1280}   \n",
       "105  {'height': 720, 'width': 1280}   \n",
       "106  {'height': 720, 'width': 1280}   \n",
       "107  {'height': 720, 'width': 1280}   \n",
       "108  {'height': 720, 'width': 1280}   \n",
       "\n",
       "                                               objects               filename  \n",
       "0    [{'id': 88945149, 'classId': 6508807, 'descrip...  b1f6c103-8b75ea3e.jpg  \n",
       "1    [{'id': 88948965, 'classId': 6508804, 'descrip...  b25fb716-78d8d49b.jpg  \n",
       "2    [{'id': 88950127, 'classId': 6508807, 'descrip...  b26f9b1e-3e172ced.jpg  \n",
       "3    [{'id': 88952159, 'classId': 6508807, 'descrip...  b289f2e8-cec8700d.jpg  \n",
       "4    [{'id': 88959006, 'classId': 6508807, 'descrip...  b313a4dc-217f1233.jpg  \n",
       "..                                                 ...                    ...  \n",
       "104  [{'id': 89256297, 'classId': 6508804, 'descrip...  c9b9fef4-a58bf3d5.jpg  \n",
       "105  [{'id': 89256888, 'classId': 6508800, 'descrip...  c9c31e57-45b42452.jpg  \n",
       "106  [{'id': 89261734, 'classId': 6508807, 'descrip...  ca28f275-03f8cef5.jpg  \n",
       "107  [{'id': 89261594, 'classId': 6508807, 'descrip...  ca28f275-cac4f254.jpg  \n",
       "108  [{'id': 89261361, 'classId': 6508800, 'descrip...  ca2ba838-fdddb1fb.jpg  \n",
       "\n",
       "[109 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_path = \"bdd100ksample/val/ann\"\n",
    "train_annotations = load_annotations(train_annotations_path)\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(train_annotations, orient='columns')\n",
    "\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c859f",
   "metadata": {},
   "source": [
    "Pregled objekata sample dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422f639",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f7cd6f3-936b-44b5-9a5f-78784470087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "objetcs_id_dict = {\n",
    "    0: 'car',\n",
    "    1: 'bus',\n",
    "    2: 'drivable area',\n",
    "    3: 'lane',\n",
    "    4: 'traffic sign',\n",
    "    5: 'truck',\n",
    "    6: 'person',\n",
    "    7: 'traffic light',\n",
    "    8: 'rider',\n",
    "    9: 'bike',\n",
    "    10:'motor',\n",
    "    11:'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9b704c0-49ee-4001-8b67-b40e036d1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totals = pd.DataFrame({\n",
    "    'car':[0],\n",
    "    'bus':[0],\n",
    "    'drivable area':[0],\n",
    "    'lane':[0],\n",
    "    'traffic sign':[0],\n",
    "    'truck':[0],\n",
    "    'person':[0],\n",
    "    'traffic light':[0],\n",
    "    'rider':[0],\n",
    "    'bike':[0],\n",
    "    'motor':[0],\n",
    "    'train':[0]\n",
    "})\n",
    "df_per_picture = pd.DataFrame({\n",
    "    'car':[0],\n",
    "    'bus':[0],\n",
    "    'drivable area':[0],\n",
    "    'lane':[0],\n",
    "    'traffic sign':[0],\n",
    "    'truck':[0],\n",
    "    'person':[0],\n",
    "    'traffic light':[0],\n",
    "    'rider':[0],\n",
    "    'bike':[0],\n",
    "    'motor':[0],\n",
    "    'train':[0]\n",
    "})\n",
    "\n",
    "for i in range(len(df2.index)): \n",
    "    picture_flags = [False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "    for o in df2.loc[i,'objects']:\n",
    "        o_id = (o['classId']-6508800)\n",
    "        df_totals.loc[0,objetcs_id_dict[o_id]] = df_totals.loc[0,objetcs_id_dict[o_id]] + 1\n",
    "        if not picture_flags[o_id]:\n",
    "            df_per_picture.loc[0,objetcs_id_dict[o_id]] = df_per_picture.loc[0,objetcs_id_dict[o_id]] + 1\n",
    "            picture_flags[o_id] = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dcd21e5-87ab-412e-8354-3f4d39edcb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>bus</th>\n",
       "      <th>drivable area</th>\n",
       "      <th>lane</th>\n",
       "      <th>traffic sign</th>\n",
       "      <th>truck</th>\n",
       "      <th>person</th>\n",
       "      <th>traffic light</th>\n",
       "      <th>rider</th>\n",
       "      <th>bike</th>\n",
       "      <th>motor</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1257</td>\n",
       "      <td>12</td>\n",
       "      <td>202</td>\n",
       "      <td>885</td>\n",
       "      <td>426</td>\n",
       "      <td>55</td>\n",
       "      <td>169</td>\n",
       "      <td>336</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    car  bus  drivable area  lane  traffic sign  truck  person  traffic light  \\\n",
       "0  1257   12            202   885           426     55     169            336   \n",
       "\n",
       "   rider  bike  motor  train  \n",
       "0      8    13     15      0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d5d629a-2107-485f-8956-fd20a9b39b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>bus</th>\n",
       "      <th>drivable area</th>\n",
       "      <th>lane</th>\n",
       "      <th>traffic sign</th>\n",
       "      <th>truck</th>\n",
       "      <th>person</th>\n",
       "      <th>traffic light</th>\n",
       "      <th>rider</th>\n",
       "      <th>bike</th>\n",
       "      <th>motor</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>11</td>\n",
       "      <td>107</td>\n",
       "      <td>105</td>\n",
       "      <td>97</td>\n",
       "      <td>37</td>\n",
       "      <td>44</td>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   car  bus  drivable area  lane  traffic sign  truck  person  traffic light  \\\n",
       "0  109   11            107   105            97     37      44             74   \n",
       "\n",
       "   rider  bike  motor  train  \n",
       "0      7    10      4      0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfef7b1",
   "metadata": {},
   "source": [
    "Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b93db",
   "metadata": {},
   "source": [
    "Transformacija anotacija u yolo format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad8b9c",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c3cdfd-0d2c-43c3-b31e-5d580aa1481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df2.index)):\n",
    "    save_location = \"bdd100ksample/train/labels/\" + df2.loc[i,'filename'].rstrip('.jpg') + \".txt\"\n",
    "    file = open(save_location,'w')\n",
    "    for o in df2.loc[i,'objects']:\n",
    "        if o['classId'] - 6508800 != 7:\n",
    "            continue\n",
    "        str_line = \"\"\n",
    "        str_line += (\"0 \")\n",
    "        str_line += (str((o['points']['exterior'][1][0]+o['points']['exterior'][0][0])/(2*1280)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][1]+o['points']['exterior'][0][1])/(2*720)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][0]-o['points']['exterior'][0][0])/(1280)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][1]-o['points']['exterior'][0][1])/(720)) + \"\\n\")\n",
    "        file.write(str_line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd78521",
   "metadata": {},
   "source": [
    "Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730bbe",
   "metadata": {},
   "source": [
    "Treniranje modela nad sample skupom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8bd546-f57f-4f4c-935e-e5c71c6f671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5a90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8487964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.24 🚀 Python-3.12.2 torch-2.2.1+cpu CPU (AMD Ryzen 7 3700U with Radeon Vega Mobile Gfx)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=bdd100ksample/data.yaml, epochs=100, time=None, patience=100, batch=2, imgsz=1280, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train22, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train22\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train22', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\train\\labels.cache... 927 images, 27 backgrounds, 0 corrupt: 1\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\val\\labels.cache... 109 images, 35 backgrounds, 0 corrupt: 100%|\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train22\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train22\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G      3.314      37.68      2.457         32       1280:   0%|          | 2/464 [00:26<1:41:06, 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbdd100ksample/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpu_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:654\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpu_env\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpu_env\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:384\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    380\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    381\u001b[0m     )\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpu_env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gpu_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = model.train(data='bdd100ksample/data.yaml', batch = 2, imgsz=1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f4f50",
   "metadata": {},
   "source": [
    "Prikaz rezultata nad test skupom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5355e45f-3726-4116-8d7f-fe6c61461483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307af4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 24461.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cac32276-a70feba7.jpg: 736x1280 (no detections), 7757.9ms\n",
      "Speed: 13.8ms preprocess, 7757.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cb879996-ecab29b0.jpg: 736x1280 (no detections), 7827.7ms\n",
      "Speed: 12.0ms preprocess, 7827.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cbb0e311-1082d7ff.jpg: 736x1280 10 traffic_lights, 7667.9ms\n",
      "Speed: 7.6ms preprocess, 7667.9ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cc236fd9-90d374e4.jpg: 736x1280 (no detections), 8237.2ms\n",
      "Speed: 11.6ms preprocess, 8237.2ms inference, 4.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cc388c9e-b41eace6.jpg: 736x1280 3 traffic_lights, 8610.2ms\n",
      "Speed: 12.7ms preprocess, 8610.2ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ccd0ec33-3386cee5.jpg: 736x1280 (no detections), 8100.5ms\n",
      "Speed: 12.1ms preprocess, 8100.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cd09a73f-5f6b9212.jpg: 736x1280 (no detections), 7938.3ms\n",
      "Speed: 10.1ms preprocess, 7938.3ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cd0c8cf0-d78ca88f.jpg: 736x1280 (no detections), 8588.7ms\n",
      "Speed: 10.2ms preprocess, 8588.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cd59c8c9-bc632097.jpg: 736x1280 (no detections), 8409.0ms\n",
      "Speed: 8.1ms preprocess, 8409.0ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cd9b086a-12d43303.jpg: 736x1280 4 traffic_lights, 10158.1ms\n",
      "Speed: 14.8ms preprocess, 10158.1ms inference, 3.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cdcc30a8-a4132413.jpg: 736x1280 (no detections), 7160.7ms\n",
      "Speed: 12.0ms preprocess, 7160.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cdcd1c99-bab60519.jpg: 736x1280 2 traffic_lights, 8954.1ms\n",
      "Speed: 0.0ms preprocess, 8954.1ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cddf367c-8c38cdcf.jpg: 736x1280 (no detections), 8607.1ms\n",
      "Speed: 13.0ms preprocess, 8607.1ms inference, 4.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ce20f7fa-bafb9098.jpg: 736x1280 (no detections), 7437.3ms\n",
      "Speed: 10.2ms preprocess, 7437.3ms inference, 10.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ce32682a-7e0156ec.jpg: 736x1280 4 traffic_lights, 7370.3ms\n",
      "Speed: 10.2ms preprocess, 7370.3ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ce786ed1-7e196be2.jpg: 736x1280 (no detections), 9208.7ms\n",
      "Speed: 12.1ms preprocess, 9208.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ced813cc-480d1c0c.jpg: 736x1280 8 traffic_lights, 7659.0ms\n",
      "Speed: 16.7ms preprocess, 7659.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cf1038c6-38badb5e.jpg: 736x1280 (no detections), 8563.8ms\n",
      "Speed: 15.6ms preprocess, 8563.8ms inference, 3.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cf3fa252-64b32820.jpg: 736x1280 3 traffic_lights, 8106.4ms\n",
      "Speed: 10.9ms preprocess, 8106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cf7f5c97-7e06dad1.jpg: 736x1280 (no detections), 7846.5ms\n",
      "Speed: 12.3ms preprocess, 7846.5ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cf917d34-acba0c6f.jpg: 736x1280 (no detections), 8067.2ms\n",
      "Speed: 10.2ms preprocess, 8067.2ms inference, 8.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cf9c863a-9c278f78.jpg: 736x1280 2 traffic_lights, 8134.7ms\n",
      "Speed: 18.4ms preprocess, 8134.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cfba56eb-39479ec3.jpg: 736x1280 3 traffic_lights, 8220.3ms\n",
      "Speed: 10.5ms preprocess, 8220.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cfcbcf90-7f9c6a78.jpg: 736x1280 8 traffic_lights, 7855.7ms\n",
      "Speed: 11.9ms preprocess, 7855.7ms inference, 10.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\cffe8287-a7e4d3f5.jpg: 736x1280 (no detections), 7525.3ms\n",
      "Speed: 11.2ms preprocess, 7525.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d047bd7a-dfa09274.jpg: 736x1280 6 traffic_lights, 7503.9ms\n",
      "Speed: 10.4ms preprocess, 7503.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d0639bd2-fcc27704.jpg: 736x1280 1 traffic_light, 8407.3ms\n",
      "Speed: 11.9ms preprocess, 8407.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d09b7eb3-bf9bfb1a.jpg: 736x1280 (no detections), 7965.9ms\n",
      "Speed: 11.7ms preprocess, 7965.9ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d178f7e5-e4bd6afb.jpg: 736x1280 (no detections), 8280.9ms\n",
      "Speed: 12.3ms preprocess, 8280.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d1e828e1-2beebb9b.jpg: 736x1280 2 traffic_lights, 7590.9ms\n",
      "Speed: 10.4ms preprocess, 7590.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d2154c1d-ab1ef472.jpg: 736x1280 3 traffic_lights, 8034.2ms\n",
      "Speed: 14.5ms preprocess, 8034.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d2216bdb-389095e2.jpg: 736x1280 (no detections), 8202.8ms\n",
      "Speed: 15.6ms preprocess, 8202.8ms inference, 1.3ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d350d794-6be69a57.jpg: 736x1280 1 traffic_light, 8266.1ms\n",
      "Speed: 11.7ms preprocess, 8266.1ms inference, 5.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d35e86f4-ad94dfe2.jpg: 736x1280 (no detections), 8150.1ms\n",
      "Speed: 11.8ms preprocess, 8150.1ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d37c935f-357dca6e.jpg: 736x1280 (no detections), 9223.1ms\n",
      "Speed: 9.7ms preprocess, 9223.1ms inference, 7.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d3cd5299-05beccf7.jpg: 736x1280 (no detections), 7697.4ms\n",
      "Speed: 12.2ms preprocess, 7697.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d4782392-9aee10b3.jpg: 736x1280 1 traffic_light, 7993.4ms\n",
      "Speed: 10.1ms preprocess, 7993.4ms inference, 0.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d49af90b-f446b6dd.jpg: 736x1280 3 traffic_lights, 8256.3ms\n",
      "Speed: 12.0ms preprocess, 8256.3ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d5027b27-c5cb186a.jpg: 736x1280 (no detections), 8322.9ms\n",
      "Speed: 12.0ms preprocess, 8322.9ms inference, 5.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d55c2727-858c710a.jpg: 736x1280 (no detections), 8831.4ms\n",
      "Speed: 10.1ms preprocess, 8831.4ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d58ae42c-3e2f6675.jpg: 736x1280 1 traffic_light, 8218.4ms\n",
      "Speed: 14.1ms preprocess, 8218.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d59b65a4-3b5b5ea4.jpg: 736x1280 6 traffic_lights, 7886.0ms\n",
      "Speed: 11.3ms preprocess, 7886.0ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d5b9be84-b755c63f.jpg: 736x1280 (no detections), 7792.0ms\n",
      "Speed: 12.3ms preprocess, 7792.0ms inference, 2.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d6b3ba2f-0212804f.jpg: 736x1280 4 traffic_lights, 7661.9ms\n",
      "Speed: 12.3ms preprocess, 7661.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d764e361-fcf8e72c.jpg: 736x1280 6 traffic_lights, 8764.7ms\n",
      "Speed: 12.0ms preprocess, 8764.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d795d9d8-2323e9d8.jpg: 736x1280 6 traffic_lights, 7529.3ms\n",
      "Speed: 13.2ms preprocess, 7529.3ms inference, 5.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d7bb963f-4bf0edd1.jpg: 736x1280 3 traffic_lights, 7743.5ms\n",
      "Speed: 15.4ms preprocess, 7743.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d7f03352-8f4da307.jpg: 736x1280 (no detections), 9199.5ms\n",
      "Speed: 12.3ms preprocess, 9199.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d83e92cf-45f97194.jpg: 736x1280 (no detections), 7662.0ms\n",
      "Speed: 10.2ms preprocess, 7662.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d86afd0b-8c6700fa.jpg: 736x1280 4 traffic_lights, 7524.9ms\n",
      "Speed: 10.4ms preprocess, 7524.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d88373bc-b4b9aae3.jpg: 736x1280 2 traffic_lights, 8189.9ms\n",
      "Speed: 12.4ms preprocess, 8189.9ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\d8e15519-567be90e.jpg: 736x1280 5 traffic_lights, 7780.0ms\n",
      "Speed: 11.0ms preprocess, 7780.0ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\da5035aa-3c834838.jpg: 736x1280 1 traffic_light, 8506.3ms\n",
      "Speed: 11.9ms preprocess, 8506.3ms inference, 3.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db145068-5049c9a4.jpg: 736x1280 2 traffic_lights, 8125.3ms\n",
      "Speed: 3.6ms preprocess, 8125.3ms inference, 6.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db24bc52-04a0d171.jpg: 736x1280 (no detections), 8178.7ms\n",
      "Speed: 9.2ms preprocess, 8178.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db55c4fe-b3b194fc.jpg: 736x1280 (no detections), 8058.0ms\n",
      "Speed: 15.6ms preprocess, 8058.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db6296e5-c12fbfc4.jpg: 736x1280 2 traffic_lights, 7288.5ms\n",
      "Speed: 10.3ms preprocess, 7288.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db6d38a1-7a5956b4.jpg: 736x1280 3 traffic_lights, 7328.2ms\n",
      "Speed: 12.2ms preprocess, 7328.2ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db6f31d4-d49858f5.jpg: 736x1280 (no detections), 8189.1ms\n",
      "Speed: 9.2ms preprocess, 8189.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\db79fe59-510c7b3b.jpg: 736x1280 (no detections), 8659.7ms\n",
      "Speed: 10.1ms preprocess, 8659.7ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dbc2bc70-2b21c210.jpg: 736x1280 4 traffic_lights, 7866.3ms\n",
      "Speed: 10.1ms preprocess, 7866.3ms inference, 3.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dbe58c0d-8e9aef50.jpg: 736x1280 3 traffic_lights, 7846.2ms\n",
      "Speed: 15.6ms preprocess, 7846.2ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dc4ff24f-118dcbee.jpg: 736x1280 (no detections), 7618.0ms\n",
      "Speed: 11.3ms preprocess, 7618.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dcc8ae84-bd15efbd.jpg: 736x1280 4 traffic_lights, 7066.9ms\n",
      "Speed: 10.1ms preprocess, 7066.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dce8f4f0-c65ccf01.jpg: 736x1280 3 traffic_lights, 7348.7ms\n",
      "Speed: 0.0ms preprocess, 7348.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dd3b1627-614774ba.jpg: 736x1280 (no detections), 7625.9ms\n",
      "Speed: 15.6ms preprocess, 7625.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dd496625-71f449ea.jpg: 736x1280 (no detections), 7922.6ms\n",
      "Speed: 10.1ms preprocess, 7922.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ddb5af91-b29d2c46.jpg: 736x1280 2 traffic_lights, 7355.4ms\n",
      "Speed: 18.3ms preprocess, 7355.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\de5864da-ac2c3a94.jpg: 736x1280 (no detections), 7091.6ms\n",
      "Speed: 12.8ms preprocess, 7091.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\de7e7619-50c4a156.jpg: 736x1280 5 traffic_lights, 7431.0ms\n",
      "Speed: 10.3ms preprocess, 7431.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\de9b0f1c-a150f646.jpg: 736x1280 (no detections), 7368.2ms\n",
      "Speed: 12.1ms preprocess, 7368.2ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\defdbb7e-08229ec4.jpg: 736x1280 4 traffic_lights, 7088.4ms\n",
      "Speed: 10.2ms preprocess, 7088.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\dfc515b2-120cef69.jpg: 736x1280 2 traffic_lights, 7379.8ms\n",
      "Speed: 11.0ms preprocess, 7379.8ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e003977b-f1fb954b.jpg: 736x1280 (no detections), 7234.4ms\n",
      "Speed: 17.1ms preprocess, 7234.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e07bde07-0304694c.jpg: 736x1280 (no detections), 7300.0ms\n",
      "Speed: 8.0ms preprocess, 7300.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e0b10e3a-fcba701f.jpg: 736x1280 5 traffic_lights, 7249.3ms\n",
      "Speed: 10.4ms preprocess, 7249.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e18b40c3-df20067d.jpg: 736x1280 (no detections), 7061.3ms\n",
      "Speed: 4.1ms preprocess, 7061.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e1a904c0-4552134a.jpg: 736x1280 1 traffic_light, 7585.4ms\n",
      "Speed: 10.3ms preprocess, 7585.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e2679ddf-c6b7b91b.jpg: 736x1280 4 traffic_lights, 7475.0ms\n",
      "Speed: 15.6ms preprocess, 7475.0ms inference, 10.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e2c8460c-f6f806f9.jpg: 736x1280 3 traffic_lights, 7667.8ms\n",
      "Speed: 11.9ms preprocess, 7667.8ms inference, 3.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e33475bd-5b03cec8.jpg: 736x1280 (no detections), 7420.7ms\n",
      "Speed: 10.1ms preprocess, 7420.7ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e3679ea3-59fa8ec7.jpg: 736x1280 6 traffic_lights, 7217.2ms\n",
      "Speed: 10.2ms preprocess, 7217.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e380ce40-b86fb203.jpg: 736x1280 7 traffic_lights, 7441.1ms\n",
      "Speed: 12.7ms preprocess, 7441.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e3820c16-82358ce7.jpg: 736x1280 (no detections), 7346.2ms\n",
      "Speed: 13.3ms preprocess, 7346.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e3993059-614d3a3d.jpg: 736x1280 1 traffic_light, 7488.1ms\n",
      "Speed: 10.2ms preprocess, 7488.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e3d90b64-57ba30bd.jpg: 736x1280 2 traffic_lights, 7392.7ms\n",
      "Speed: 8.1ms preprocess, 7392.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e4644034-b0facd9d.jpg: 736x1280 (no detections), 7745.6ms\n",
      "Speed: 12.1ms preprocess, 7745.6ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e46ed17b-9784a55d.jpg: 736x1280 (no detections), 7151.0ms\n",
      "Speed: 10.1ms preprocess, 7151.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e46ed17b-aa6fa901.jpg: 736x1280 (no detections), 7507.5ms\n",
      "Speed: 8.2ms preprocess, 7507.5ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e473f25e-90e20efa.jpg: 736x1280 1 traffic_light, 7565.7ms\n",
      "Speed: 10.2ms preprocess, 7565.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e4b3abca-fcf5323b.jpg: 736x1280 11 traffic_lights, 7222.5ms\n",
      "Speed: 0.0ms preprocess, 7222.5ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e4b97ca2-dbca9e47.jpg: 736x1280 4 traffic_lights, 7786.9ms\n",
      "Speed: 14.9ms preprocess, 7786.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e4fb4514-944f3e03.jpg: 736x1280 (no detections), 7684.2ms\n",
      "Speed: 12.0ms preprocess, 7684.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e56de4f5-3f4f9abb.jpg: 736x1280 3 traffic_lights, 8246.5ms\n",
      "Speed: 10.2ms preprocess, 8246.5ms inference, 3.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e5702ae4-c4f102ec.jpg: 736x1280 (no detections), 7302.0ms\n",
      "Speed: 12.5ms preprocess, 7302.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e60966a3-1d819221.jpg: 736x1280 3 traffic_lights, 7056.2ms\n",
      "Speed: 12.4ms preprocess, 7056.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e6bdca9e-cb2a2981.jpg: 736x1280 9 traffic_lights, 7347.1ms\n",
      "Speed: 6.1ms preprocess, 7347.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e6c5ef8f-fd2d6707.jpg: 736x1280 2 traffic_lights, 7285.1ms\n",
      "Speed: 13.6ms preprocess, 7285.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e72419b3-26717666.jpg: 736x1280 (no detections), 7131.7ms\n",
      "Speed: 11.7ms preprocess, 7131.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e741203a-c5146f81.jpg: 736x1280 (no detections), 6785.7ms\n",
      "Speed: 10.1ms preprocess, 6785.7ms inference, 8.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e764d486-6b20a732.jpg: 736x1280 (no detections), 7177.0ms\n",
      "Speed: 12.5ms preprocess, 7177.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e7970d50-488fb8af.jpg: 736x1280 (no detections), 6961.7ms\n",
      "Speed: 10.2ms preprocess, 6961.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e7cf686b-302153e8.jpg: 736x1280 (no detections), 7465.4ms\n",
      "Speed: 10.1ms preprocess, 7465.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e7e5cbf4-ddcc4dae.jpg: 736x1280 (no detections), 7776.5ms\n",
      "Speed: 10.3ms preprocess, 7776.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e818f8ce-4bc1a302.jpg: 736x1280 1 traffic_light, 7530.1ms\n",
      "Speed: 10.1ms preprocess, 7530.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e8b97ba3-d3b75ac4.jpg: 736x1280 3 traffic_lights, 7564.3ms\n",
      "Speed: 15.6ms preprocess, 7564.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e935aa87-8c103eb1.jpg: 736x1280 (no detections), 7544.1ms\n",
      "Speed: 7.5ms preprocess, 7544.1ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e9531c3c-ef91ad3b.jpg: 736x1280 3 traffic_lights, 7449.3ms\n",
      "Speed: 10.3ms preprocess, 7449.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e9776261-a40b0e74.jpg: 736x1280 (no detections), 7472.0ms\n",
      "Speed: 15.7ms preprocess, 7472.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e98ceed7-c2592434.jpg: 736x1280 2 traffic_lights, 7475.8ms\n",
      "Speed: 10.2ms preprocess, 7475.8ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\e9caefde-83ef5ae0.jpg: 736x1280 1 traffic_light, 7283.6ms\n",
      "Speed: 11.1ms preprocess, 7283.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ea1e7f3f-b38cbe99.jpg: 736x1280 (no detections), 7325.8ms\n",
      "Speed: 12.3ms preprocess, 7325.8ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ea238aca-fb4bb3a2.jpg: 736x1280 1 traffic_light, 7603.8ms\n",
      "Speed: 10.2ms preprocess, 7603.8ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ea244d67-61de2bfd.jpg: 736x1280 (no detections), 7386.6ms\n",
      "Speed: 10.1ms preprocess, 7386.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ea871971-f28e4639.jpg: 736x1280 9 traffic_lights, 7583.1ms\n",
      "Speed: 10.2ms preprocess, 7583.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ea9b012a-a9bbb53f.jpg: 736x1280 4 traffic_lights, 7586.0ms\n",
      "Speed: 11.1ms preprocess, 7586.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ebdff001-07ecebc6.jpg: 736x1280 (no detections), 8062.9ms\n",
      "Speed: 13.0ms preprocess, 8062.9ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ec038793-8da405b4.jpg: 736x1280 3 traffic_lights, 8047.4ms\n",
      "Speed: 13.6ms preprocess, 8047.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ec04b3ee-bd457fc8.jpg: 736x1280 (no detections), 8334.5ms\n",
      "Speed: 15.7ms preprocess, 8334.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ec1a5ec5-0e77659e.jpg: 736x1280 (no detections), 8432.9ms\n",
      "Speed: 8.5ms preprocess, 8432.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ec6edb5d-334ee84c.jpg: 736x1280 1 traffic_light, 8359.7ms\n",
      "Speed: 12.0ms preprocess, 8359.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ec6edb5d-fb3133b8.jpg: 736x1280 3 traffic_lights, 8284.3ms\n",
      "Speed: 19.6ms preprocess, 8284.3ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed0d1fd7-824af8e1.jpg: 736x1280 (no detections), 8248.1ms\n",
      "Speed: 12.1ms preprocess, 8248.1ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed539d21-742a70fb.jpg: 736x1280 (no detections), 8606.2ms\n",
      "Speed: 16.7ms preprocess, 8606.2ms inference, 2.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed6ae5cc-db658e93.jpg: 736x1280 3 traffic_lights, 8047.3ms\n",
      "Speed: 18.5ms preprocess, 8047.3ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed6dca43-4ea30a7c.jpg: 736x1280 (no detections), 7213.7ms\n",
      "Speed: 12.0ms preprocess, 7213.7ms inference, 8.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed884222-fdd7f4e6.jpg: 736x1280 (no detections), 6913.9ms\n",
      "Speed: 10.2ms preprocess, 6913.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed956c57-3181e702.jpg: 736x1280 7 traffic_lights, 7499.1ms\n",
      "Speed: 10.2ms preprocess, 7499.1ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ed9c104c-869f69f2.jpg: 736x1280 (no detections), 7870.5ms\n",
      "Speed: 12.6ms preprocess, 7870.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ee00a695-588a1337.jpg: 736x1280 (no detections), 7554.5ms\n",
      "Speed: 10.1ms preprocess, 7554.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ee745e65-c6e2e62f.jpg: 736x1280 (no detections), 7623.8ms\n",
      "Speed: 10.2ms preprocess, 7623.8ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\eeae826b-5edc1314.jpg: 736x1280 1 traffic_light, 7545.4ms\n",
      "Speed: 10.2ms preprocess, 7545.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\eeed75b0-f9063b09.jpg: 736x1280 4 traffic_lights, 7817.9ms\n",
      "Speed: 12.0ms preprocess, 7817.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ef04ce5c-c1bff15e.jpg: 736x1280 (no detections), 7326.1ms\n",
      "Speed: 9.0ms preprocess, 7326.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ef0e8d15-d8929c4e.jpg: 736x1280 (no detections), 7443.2ms\n",
      "Speed: 10.3ms preprocess, 7443.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ef3d70eb-afc95d53.jpg: 736x1280 11 traffic_lights, 7534.3ms\n",
      "Speed: 10.2ms preprocess, 7534.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\ef618480-85e1f205.jpg: 736x1280 1 traffic_light, 7150.9ms\n",
      "Speed: 12.2ms preprocess, 7150.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\efa2ee4c-6406c6c0.jpg: 736x1280 4 traffic_lights, 7263.5ms\n",
      "Speed: 11.0ms preprocess, 7263.5ms inference, 3.3ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\efb139a3-44ea3234.jpg: 736x1280 (no detections), 7371.6ms\n",
      "Speed: 12.7ms preprocess, 7371.6ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\effd53cf-1ed4edae.jpg: 736x1280 (no detections), 7254.4ms\n",
      "Speed: 10.2ms preprocess, 7254.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f0056341-503d57a3.jpg: 736x1280 (no detections), 7270.1ms\n",
      "Speed: 10.2ms preprocess, 7270.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f01ef838-ecf6e8d3.jpg: 736x1280 4 traffic_lights, 7263.2ms\n",
      "Speed: 10.1ms preprocess, 7263.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f041b074-43181ffa.jpg: 736x1280 (no detections), 8025.5ms\n",
      "Speed: 11.5ms preprocess, 8025.5ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f04f4013-65c68450.jpg: 736x1280 3 traffic_lights, 8279.1ms\n",
      "Speed: 15.6ms preprocess, 8279.1ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f0e21627-1b9b7074.jpg: 736x1280 (no detections), 8049.5ms\n",
      "Speed: 11.0ms preprocess, 8049.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f1a2d81c-5c4953e2.jpg: 736x1280 8 traffic_lights, 7919.5ms\n",
      "Speed: 18.4ms preprocess, 7919.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f1a9f44b-124d15fa.jpg: 736x1280 8 traffic_lights, 8199.5ms\n",
      "Speed: 12.3ms preprocess, 8199.5ms inference, 4.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f1e9a274-de97b201.jpg: 736x1280 (no detections), 8172.5ms\n",
      "Speed: 16.9ms preprocess, 8172.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f224385a-88ca0ab0.jpg: 736x1280 7 traffic_lights, 7136.6ms\n",
      "Speed: 18.5ms preprocess, 7136.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f2d410f9-aad22f12.jpg: 736x1280 2 traffic_lights, 7714.5ms\n",
      "Speed: 11.6ms preprocess, 7714.5ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f2e65d2f-d87266e7.jpg: 736x1280 (no detections), 7714.0ms\n",
      "Speed: 17.1ms preprocess, 7714.0ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f32722a1-440bf5f3.jpg: 736x1280 (no detections), 7697.6ms\n",
      "Speed: 10.3ms preprocess, 7697.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f3398cf8-e11b2398.jpg: 736x1280 (no detections), 8795.6ms\n",
      "Speed: 10.2ms preprocess, 8795.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f3a8fbe8-79191887.jpg: 736x1280 (no detections), 7569.3ms\n",
      "Speed: 10.2ms preprocess, 7569.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f3aaa991-22fde189.jpg: 736x1280 2 traffic_lights, 8629.1ms\n",
      "Speed: 21.0ms preprocess, 8629.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f3be8009-47a0b3f7.jpg: 736x1280 7 traffic_lights, 7507.2ms\n",
      "Speed: 12.4ms preprocess, 7507.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f40746b2-71e46c16.jpg: 736x1280 1 traffic_light, 8754.6ms\n",
      "Speed: 11.8ms preprocess, 8754.6ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f41dcccc-0100b062.jpg: 736x1280 1 traffic_light, 7789.4ms\n",
      "Speed: 12.8ms preprocess, 7789.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f4518f90-f38a41c3.jpg: 736x1280 (no detections), 7580.3ms\n",
      "Speed: 15.0ms preprocess, 7580.3ms inference, 3.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f455c1ed-9260af59.jpg: 736x1280 (no detections), 7120.4ms\n",
      "Speed: 10.2ms preprocess, 7120.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f522c4b8-aa3d5870.jpg: 736x1280 1 traffic_light, 8185.1ms\n",
      "Speed: 10.1ms preprocess, 8185.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f5407be2-71b6111c.jpg: 736x1280 (no detections), 7657.7ms\n",
      "Speed: 11.1ms preprocess, 7657.7ms inference, 1.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f5833916-325bf7d7.jpg: 736x1280 (no detections), 7382.1ms\n",
      "Speed: 10.2ms preprocess, 7382.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f5e8e5a7-0773b2ba.jpg: 736x1280 2 traffic_lights, 7362.3ms\n",
      "Speed: 10.3ms preprocess, 7362.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f64efbe9-7cbf9f14.jpg: 736x1280 (no detections), 7598.3ms\n",
      "Speed: 12.0ms preprocess, 7598.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f66ac7bb-dc7d898f.jpg: 736x1280 1 traffic_light, 7009.3ms\n",
      "Speed: 10.2ms preprocess, 7009.3ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f6920aca-d46c849f.jpg: 736x1280 4 traffic_lights, 8382.2ms\n",
      "Speed: 11.1ms preprocess, 8382.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f6b3a7d6-fe2aed24.jpg: 736x1280 8 traffic_lights, 7674.7ms\n",
      "Speed: 11.7ms preprocess, 7674.7ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f72034dd-2012fe1c.jpg: 736x1280 1 traffic_light, 7692.4ms\n",
      "Speed: 12.2ms preprocess, 7692.4ms inference, 10.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f7b2e6b8-9710b7aa.jpg: 736x1280 (no detections), 7713.7ms\n",
      "Speed: 15.7ms preprocess, 7713.7ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f80854ca-a6308e25.jpg: 736x1280 7 traffic_lights, 7284.5ms\n",
      "Speed: 18.5ms preprocess, 7284.5ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f8251da4-e8943fd7.jpg: 736x1280 (no detections), 7468.8ms\n",
      "Speed: 12.0ms preprocess, 7468.8ms inference, 8.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f829504d-51c1f613.jpg: 736x1280 (no detections), 7550.1ms\n",
      "Speed: 10.1ms preprocess, 7550.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f8767c91-5aa8345e.jpg: 736x1280 (no detections), 7387.1ms\n",
      "Speed: 10.6ms preprocess, 7387.1ms inference, 0.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f887ea51-68ff945e.jpg: 736x1280 (no detections), 7095.1ms\n",
      "Speed: 10.1ms preprocess, 7095.1ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f8acb7a6-1ea92ace.jpg: 736x1280 (no detections), 6935.1ms\n",
      "Speed: 10.1ms preprocess, 6935.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f950fbec-c7db9046.jpg: 736x1280 (no detections), 7459.5ms\n",
      "Speed: 10.1ms preprocess, 7459.5ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f9798bd8-b5a65404.jpg: 736x1280 3 traffic_lights, 7681.3ms\n",
      "Speed: 18.3ms preprocess, 7681.3ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f9a1bbb6-51f869a4.jpg: 736x1280 1 traffic_light, 7974.1ms\n",
      "Speed: 0.0ms preprocess, 7974.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f9d352c5-a63b8c68.jpg: 736x1280 (no detections), 7318.8ms\n",
      "Speed: 8.8ms preprocess, 7318.8ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\f9d7d3ee-927bbf3f.jpg: 736x1280 (no detections), 7674.5ms\n",
      "Speed: 10.2ms preprocess, 7674.5ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fa3a71c8-5202228f.jpg: 736x1280 1 traffic_light, 7635.4ms\n",
      "Speed: 10.1ms preprocess, 7635.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fa64a6b2-57ba48c0.jpg: 736x1280 (no detections), 7300.6ms\n",
      "Speed: 8.1ms preprocess, 7300.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fa983dd9-335be413.jpg: 736x1280 (no detections), 7080.2ms\n",
      "Speed: 10.3ms preprocess, 7080.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fad73dbd-93936f81.jpg: 736x1280 1 traffic_light, 7091.1ms\n",
      "Speed: 2.1ms preprocess, 7091.1ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fb1c62a6-355006da.jpg: 736x1280 10 traffic_lights, 7360.2ms\n",
      "Speed: 5.1ms preprocess, 7360.2ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fb6cddef-8a87f68d.jpg: 736x1280 (no detections), 7329.0ms\n",
      "Speed: 8.1ms preprocess, 7329.0ms inference, 3.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fbc316dd-276bc685.jpg: 736x1280 (no detections), 7581.9ms\n",
      "Speed: 18.0ms preprocess, 7581.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc07772f-9aea45f5.jpg: 736x1280 (no detections), 7089.4ms\n",
      "Speed: 10.2ms preprocess, 7089.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc28e886-20b3eba0.jpg: 736x1280 1 traffic_light, 6967.5ms\n",
      "Speed: 10.4ms preprocess, 6967.5ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc3ccd74-4e75a461.jpg: 736x1280 (no detections), 8098.3ms\n",
      "Speed: 12.0ms preprocess, 8098.3ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc5233bc-20747478.jpg: 736x1280 7 traffic_lights, 7495.2ms\n",
      "Speed: 12.4ms preprocess, 7495.2ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc5af548-51f7aa40.jpg: 736x1280 13 traffic_lights, 7331.1ms\n",
      "Speed: 15.8ms preprocess, 7331.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fc83bd30-94707ca6.jpg: 736x1280 (no detections), 7268.4ms\n",
      "Speed: 0.0ms preprocess, 7268.4ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fcd52564-f5e8e8f3.jpg: 736x1280 (no detections), 6926.9ms\n",
      "Speed: 10.3ms preprocess, 6926.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fd13c13b-3f52c830.jpg: 736x1280 (no detections), 7576.9ms\n",
      "Speed: 10.2ms preprocess, 7576.9ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fd313afa-a55c12da.jpg: 736x1280 6 traffic_lights, 7329.4ms\n",
      "Speed: 10.3ms preprocess, 7329.4ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fd6a0bfc-7e177269.jpg: 736x1280 (no detections), 7234.4ms\n",
      "Speed: 12.9ms preprocess, 7234.4ms inference, 8.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "image 1/1 C:\\Users\\these\\Desktop\\tem\\bdd100ksample\\test\\images\\fd90b5f3-bbbca9a6.jpg: 736x1280 (no detections), 7391.4ms\n",
      "Speed: 10.2ms preprocess, 7391.4ms inference, 2.0ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "model = YOLO('mod2.pt')\n",
    "for img in tqdm(os.listdir( 'bdd100ksample/test/images')):\n",
    "    img_path = os.path.join('bdd100ksample/test/images', img)\n",
    "    imgs.append(img_path)\n",
    "    \n",
    "for img in imgs:\n",
    "    imgr = cv2.imread(img,1)\n",
    "    results = model(img)[0]\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "        cv2.rectangle(imgr, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "        cv2.putText(imgr, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        cv2.imshow('',imgr)\n",
    "        if cv2.waitKey(0) == ord('q'):\n",
    "            break\n",
    "    if cv2.waitKey(0) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break      \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28831e10",
   "metadata": {},
   "source": [
    "Generisanje datafrejma - ceo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc36e8",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde27da9-8e30-493a-8978-23227ad10b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 69736/69736 [02:17<00:00, 506.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_annotations_path = \"bdd100k/train/ann\"\n",
    "train_annotations = load_annotations(train_annotations_path)\n",
    "\n",
    "df = pd.DataFrame.from_dict(train_annotations, orient='columns')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totals2 = pd.DataFrame({\n",
    "    'car':[0],\n",
    "    'bus':[0],\n",
    "    'drivable area':[0],\n",
    "    'lane':[0],\n",
    "    'traffic sign':[0],\n",
    "    'truck':[0],\n",
    "    'person':[0],\n",
    "    'traffic light':[0],\n",
    "    'rider':[0],\n",
    "    'bike':[0],\n",
    "    'motor':[0],\n",
    "    'train':[0]\n",
    "})\n",
    "df_per_picture2 = pd.DataFrame({\n",
    "    'car':[0],\n",
    "    'bus':[0],\n",
    "    'drivable area':[0],\n",
    "    'lane':[0],\n",
    "    'traffic sign':[0],\n",
    "    'truck':[0],\n",
    "    'person':[0],\n",
    "    'traffic light':[0],\n",
    "    'rider':[0],\n",
    "    'bike':[0],\n",
    "    'motor':[0],\n",
    "    'train':[0]\n",
    "})\n",
    "\n",
    "for i in range(len(df.index)): \n",
    "    picture_flags = [False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "    for o in df.loc[i,'objects']:\n",
    "        o_id = (o['classId']-6508800)\n",
    "        df_totals2.loc[0,objetcs_id_dict[o_id]] = df_totals2.loc[0,objetcs_id_dict[o_id]] + 1\n",
    "        if not picture_flags[o_id]:\n",
    "            df_per_picture2.loc[0,objetcs_id_dict[o_id]] = df_per_picture2.loc[0,objetcs_id_dict[o_id]] + 1\n",
    "            picture_flags[o_id] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c379e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_picture2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b1812",
   "metadata": {},
   "source": [
    "Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406e228",
   "metadata": {},
   "source": [
    "Transformacija sample dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca2b76",
   "metadata": {},
   "source": [
    "1. Brisanje svih slika i anotacija koje ne sadrze semafor u sebi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54ca160-1063-4787-9a20-8c51d0bf7d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 69736/69736 [00:00<00:00, 757942.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jpg_dict = {}\n",
    "\n",
    "for picture in tqdm(os.listdir('bdd100ksample/train/images')):\n",
    "       jpg_dict[picture] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a113d2-ed0e-4777-8cb2-ea9661edbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in df2.index:\n",
    "    for obj in df2['objects'][ind]:\n",
    "        if (obj['classId']-6508800)==7:\n",
    "            jpg_dict[df2['filename'][ind]] = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24824091-948c-480d-b376-e36ab33f186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in jpg_dict:\n",
    "    if not jpg_dict[pair]:\n",
    "        os.remove('bdd100k/train/ann/'+pair+'.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdc1de",
   "metadata": {},
   "source": [
    "2. Kopiranje slika iz celog dataseta u sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d2663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69736\n"
     ]
    }
   ],
   "source": [
    "jpg_dict = {}\n",
    "\n",
    "for ind in df.index:\n",
    "    jpg_dict[df['filename'][ind]] = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for picture in tqdm(os.listdir('bdd100ksample/train/images')):\n",
    "       jpg_dict[picture] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "768ce3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "sem_picture_num = 900-435\n",
    "no_sem_picture_num = 27\n",
    "for ind in df.index:\n",
    "    pic_filename = df['filename'][ind]\n",
    "    \n",
    "    if jpg_dict[pic_filename]:\n",
    "        continue\n",
    "        \n",
    "    is_sem_pic = False\n",
    "    for obj in df['objects'][ind]:\n",
    "        if (obj['classId']-6508800)==7:\n",
    "            shutil.copyfile('bdd100k/train/ann/'+pic_filename+'.json', 'bdd100ksample/train/ann/'+pic_filename+'.json')\n",
    "            shutil.copyfile('bdd100k/train/images/'+pic_filename, 'bdd100ksample/train/images/'+pic_filename)\n",
    "            sem_picture_num-=1\n",
    "            is_sem_pic = True\n",
    "            break\n",
    "    if not is_sem_pic and no_sem_picture_num > 0:\n",
    "        shutil.copyfile('bdd100k/train/ann/'+pic_filename+'.json', 'bdd100ksample/train/ann/'+pic_filename+'.json')\n",
    "        shutil.copyfile('bdd100k/train/images/'+pic_filename, 'bdd100ksample/train/images/'+pic_filename)\n",
    "        no_sem_picture_num-=1\n",
    "    if no_sem_picture_num==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6f8b7",
   "metadata": {},
   "source": [
    "Prikaz rezultata sa poboljsanim sample datasetom ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd85fb",
   "metadata": {},
   "source": [
    "Transformacija celog dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9c7c8",
   "metadata": {},
   "source": [
    "1. Smanjivanje broja slika bez semafora na 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_sem = 1000\n",
    "for ind in df.index:\n",
    "    to_delete = True\n",
    "    for obj in df['objects'][ind]:\n",
    "        if (obj['classId']-6508800)==7:\n",
    "            to_delete = False\n",
    "            break\n",
    "    if total_no_sem>0:\n",
    "        total_no_sem -=1\n",
    "        to_delete = False\n",
    "    if to_delete:\n",
    "        os.remove('bdd100k/train/ann/'+pair+'.json')\n",
    "        os.remove('bdd100k/train/images/'+pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb7045",
   "metadata": {},
   "source": [
    "ponovno ucitavanje dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_path = \"bdd100k/train/ann\"\n",
    "train_annotations = load_annotations(train_annotations_path)\n",
    "\n",
    "df = pd.DataFrame.from_dict(train_annotations, orient='columns')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5c98a",
   "metadata": {},
   "source": [
    "2. Transformacija u yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a866d180-701f-46e3-ae42-e7f171145b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#srediti jos\n",
    "\n",
    "for i in range(len(df.index)):\n",
    "    save_location = \"bdd100k/train/labels/\" + df.loc[i,'filename'].rstrip('.jpg') + \".txt\"\n",
    "    file = open(save_location,'w')\n",
    "    for o in df.loc[i,'objects']:\n",
    "        if o['classId'] - 6508800 != 7:\n",
    "            continue\n",
    "        str_line = \"\"\n",
    "        str_line += (\"0 \")\n",
    "        str_line += (str((o['points']['exterior'][1][0]+o['points']['exterior'][0][0])/(2*1280)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][1]+o['points']['exterior'][0][1])/(2*720)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][0]-o['points']['exterior'][0][0])/(1280)) + \" \")\n",
    "        str_line += (str((o['points']['exterior'][1][1]-o['points']['exterior'][0][1])/(720)) + \"\\n\")\n",
    "        file.write(str_line)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bd8a",
   "metadata": {},
   "source": [
    "Prikaz rezultata nad celim skupom..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9b6d538-be9c-4f4b-a650-296c8b16f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.index)): \n",
    "    img = cv2.imread(\"bdd100k/train/images/\"+df.loc[i,'filename'],1)\n",
    "   # print(i)\n",
    "    for o in df.loc[i,'objects']:\n",
    "        o_id = (o['classId']-6508800)\n",
    "        if o_id != 7:\n",
    "            continue\n",
    "        cv2.rectangle(img,(o['points']['exterior'][0][0],o['points']['exterior'][0][1]),(o['points']['exterior'][1][0],o['points']['exterior'][1][1]),(0,0,255),2)\n",
    "        cv2.imshow(df.loc[i,'filename'],img)\n",
    "        if cv2.waitKey(0) == ord('q'):\n",
    "            break\n",
    "    if cv2.waitKey(0) == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break      \n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060b6683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 (no detections), 7847.1ms\n",
      "Speed: 20.3ms preprocess, 7847.1ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 (no detections), 7451.6ms\n",
      "Speed: 14.4ms preprocess, 7451.6ms inference, 0.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 (no detections), 7928.8ms\n",
      "Speed: 20.2ms preprocess, 7928.8ms inference, 2.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 (no detections), 7907.9ms\n",
      "Speed: 10.2ms preprocess, 7907.9ms inference, 7.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ret:\n\u001b[1;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtolist():\n\u001b[0;32m     29\u001b[0m         x1, y1, x2, y2, score, class_id \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\engine\\model.py:169\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    148\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    149\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\engine\\model.py:429\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 283\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\engine\\predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    136\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:83\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:101\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\tasks.py:122\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    123\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:225\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 225\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:225\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 225\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:335\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124;03m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "VIDEOS_DIR = os.path.join('.', 'videos')\n",
    "\n",
    "video_path = 'vid5.mp4'\n",
    "video_path_out = '{}_out.mp4'.format(video_path)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "H, W, _ = frame.shape\n",
    "out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*'MP4V'), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))\n",
    "\n",
    "\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('mod.pt')  # load a custom model\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "while ret:\n",
    "\n",
    "    results = model(frame)[0]\n",
    "\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "        if score > threshold:\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "            cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    out.write(frame)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda346ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
